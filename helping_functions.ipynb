{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90b393e2-c08f-4a47-b2b2-a7f21b4532d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba186c-bc85-4572-81d2-ba59ed6e0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Regular expression pattern to extract float values\n",
    "float_pattern = r'tf\\.Tensor\\(([-+]?\\d*\\.\\d+|\\d+),.*\\)'  \n",
    "\n",
    "# Function to extract float value from a tensor or use float directly\n",
    "def extract_float_value(value):\n",
    "    match = re.match(float_pattern, str(value))\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        return float(value)\n",
    "\n",
    "def plot_multiple_csv(directory_path, column_name, window_size, start_round, end_round, task, pattern, algorithm):\n",
    "    if task == 'epochs' or task =='cohorts':\n",
    "        csv_files = [file for file in os.listdir(directory_path) if file.startswith(algorithm) and 'validation_data.csv' in file ]\n",
    "    elif task == 'algorithms':\n",
    "        csv_files = [file for file in os.listdir(directory_path) if pattern in file and 'validation_data.csv' in file ]\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Define a color mapping dictionary\n",
    "    color_mapping = {}\n",
    "\n",
    "    # Collect legend labels and sort them\n",
    "    if task == 'epochs':\n",
    "        legend_labels = sorted((re.search(r'_e(\\d+)_', csv_file).group(1) for csv_file in csv_files if re.search(r'_e\\d+_', csv_file)), key=lambda x: int(x))    \n",
    "        legend_prefix=\"E:\"\n",
    "    elif task == 'cohorts':\n",
    "        legend_labels = sorted((re.search(r'_c(\\d+)_', csv_file).group(1) for csv_file in csv_files if re.search(r'_c\\d+_', csv_file)), key=lambda x: int(x))  \n",
    "        legend_prefix=\"C:\"\n",
    "    elif task == 'algorithms':\n",
    "        legend_labels = sorted((re.search(r'^([a-zA-Z]+)_', csv_file).group(1) for csv_file in csv_files if re.search(r'^[a-zA-Z]+_', csv_file)), key=str)\n",
    "        legend_prefix=\"A:\"\n",
    "\n",
    "    for legend_label in legend_labels:\n",
    "        # Find the corresponding CSV file for the current legend label\n",
    "        matching_csv_file = next(csv_file for csv_file in csv_files if legend_label+\"_\" in csv_file)\n",
    "        print(f'Legend Label: {legend_label}, Matching CSV File: {matching_csv_file}')\n",
    "\n",
    "        data = pd.read_csv(os.path.join(directory_path, matching_csv_file), nrows=int(total_rounds / rounds_per_eval))\n",
    "        output_filename = os.path.splitext(matching_csv_file)[0] + \"_\" + column_name + \".jpg\"\n",
    "        data_column = data[column_name]  \n",
    "        data_column = data_column.apply(extract_float_value)\n",
    "\n",
    "        if len(data_column) < total_rounds:\n",
    "            acc_points = int(total_rounds / rounds_per_eval)\n",
    "            accuracy_series = pd.Series(data_column)\n",
    "        else:\n",
    "            acc_points = len(data_column)\n",
    "            accuracy_series = data_column\n",
    "\n",
    "        x_values = [i * rounds_per_eval for i in range(acc_points)]\n",
    "        x_series = pd.Series(x_values[:acc_points])\n",
    "        moving_average = accuracy_series.rolling(window=window_size).mean()\n",
    "        new_data = np.concatenate([accuracy_series.head(window_size), moving_average])\n",
    "        new_data=new_data[~np.isnan(new_data)]\n",
    "        moving_average=moving_average[~np.isnan(moving_average)]\n",
    "\n",
    "        # Assign a color to the legend label if not already assigned\n",
    "        if legend_label not in color_mapping:\n",
    "            color_mapping[legend_label] = plt.colormaps.get_cmap('tab10')(len(color_mapping) % 10)\n",
    "\n",
    "        line_color = color_mapping[legend_label]\n",
    "\n",
    "        # Plot only the moving average line within the specified range\n",
    "        plt.plot(x_series[start_round // rounds_per_eval:end_round // rounds_per_eval], \n",
    "                 new_data[start_round // rounds_per_eval:end_round // rounds_per_eval], \n",
    "                 label=f'{legend_prefix+legend_label}', linestyle='-', marker='o', alpha=0.7, color=line_color)\n",
    "\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel(column_name)\n",
    "    plt.title(f'Validation {column_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(target_dir + \"/\" + algorithm + '_'+ dataset + '_'+pattern+task+'.jpg')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "algorithms = [\"fedAvg\", \"fedAdadb\", \"fedAdam\"]\n",
    "target_dir = \"figures/\"\n",
    "pattern = \"c10_e4_\" # e4_ / c5_ / c10_e4_\n",
    "task = 'algorithms' #cohorts / epochs / algorithms\n",
    "dataset=\"emnist\" #emnist / shakespeare\n",
    "total_rounds = 2000\n",
    "rounds_per_eval = 5\n",
    "\n",
    "%rm -f $target_dir*.csv\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    source_path = \"results/official/\"+dataset+\"/\"+algorithm+\"/training\"\n",
    "    \n",
    "    # List files matching the pattern in the source directory\n",
    "    matching_files = [f for f in os.listdir(source_path) if pattern in f and 'validation_data.csv' in f]\n",
    "    \n",
    "    # Create target directory if it doesn't exist\n",
    "    target_algorithm_dir = os.path.join(target_dir)\n",
    "    \n",
    "    # Copy matching files to the target directory\n",
    "    for file_name in matching_files:\n",
    "        source_file_path = os.path.join(source_path, file_name)\n",
    "        target_file_path = os.path.join(target_algorithm_dir, algorithm + \"_\" +file_name)\n",
    "        shutil.copy(source_file_path, target_file_path)\n",
    "\n",
    "    # Usage: Specify start_round and end_round parameters\n",
    "    if task != 'algorithms':\n",
    "        plot_multiple_csv(target_dir, 'Accuracy', 1, 5, 250, task, pattern, algorithm)\n",
    "        #plot_multiple_csv(target_dir, 'Accuracy', 5, 0, 5000)\n",
    "\n",
    "if task == 'algorithms':\n",
    "    plot_multiple_csv(target_dir, 'Accuracy', 1, 5, 250, task, pattern, 'all')\n",
    "\n",
    "\n",
    "%rm -f $target_dir*.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a7c47-0f14-46bf-bf77-c1052f18deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "\n",
    "algorithms = [\"fedAvg\", \"fedAdam\", \"fedAdadb\"]\n",
    "dataset=\"shakespeare\"\n",
    "total_rounds = 250\n",
    "rounds_per_eval = 5\n",
    "cohorts=[5,10,50]\n",
    "epochs=[1,2,4,8,16]\n",
    "c1,c2=0,0\n",
    "th1=40 #40 shakespeare 85 emnist\n",
    "th2=50 #55 shakespeare 99 emnist\n",
    "last_rounds=10 # 10 shakespeare 100 emnist\n",
    "\n",
    "# Regular expression pattern to extract float values\n",
    "float_pattern = r'tf\\.Tensor\\(([-+]?\\d*\\.\\d+|\\d+),.*\\)'  \n",
    "\n",
    "# Function to extract float value from a tensor or use float directly\n",
    "def extract_float_value(value):\n",
    "    match = re.match(float_pattern, str(value))\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        return float(value)\n",
    "\n",
    "def get_avg_metrics(csv_file, column_name, metric, window_size, convergence_percentage=None):\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract data from the specified column\n",
    "    data_column = data[column_name]\n",
    "    data_column = data_column.apply(extract_float_value)\n",
    "\n",
    "    if metric == \"convergence\":\n",
    "        # Calculate the moving average line\n",
    "        moving_average = data_column.rolling(window=window_size).mean()\n",
    "    \n",
    "        # Find the round where the moving average crosses target convergence percentage\n",
    "        metrics = next((i for i, avg in enumerate(moving_average) if avg > convergence_percentage), None)\n",
    "    elif metric == \"avgAcc\":\n",
    "        # Calculate the average accuracy over window_size\n",
    "        metrics = data_column.tail(window_size).mean()\n",
    "    return metrics\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    source_path = \"results/official/\"+dataset+\"/\"+algorithm+\"/training\"\n",
    "    for c in cohorts:\n",
    "        for e in epochs:\n",
    "            pattern=\"c\"+str(c)+\"_e\"+str(e)+\"_\"\n",
    "            file_path = os.path.join(source_path, pattern + 'training_settings.txt')\n",
    "\n",
    "            # Check if the specific file exists\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as file:\n",
    "                    # Iterate over each line in the file\n",
    "                    for line in file:\n",
    "                        # Split the line into key and value using ':' as delimiter\n",
    "                        key, value = line.strip().split(': ')\n",
    "                        # Check if the key is 'client_learning_rate'\n",
    "                        if key == 'client_learning_rate':\n",
    "                            # Assign the value to the variable\n",
    "                            n = float(value)\n",
    "                            # Exit the loop as we found the value\n",
    "                        elif key == 'server_learning_rate':\n",
    "                            ns = float(value)\n",
    "            else:\n",
    "                # Search for any matching file\n",
    "                wildcard_path = os.path.join(source_path, '*' + 'training_settings.txt')\n",
    "                matching_files = glob.glob(wildcard_path)\n",
    "                \n",
    "                if matching_files:\n",
    "                    # Open the first matching file\n",
    "                    with open(matching_files[0], 'r') as file:\n",
    "                        # Iterate over each line in the file\n",
    "                        for line in file:\n",
    "                            # Split the line into key and value using ':' as delimiter\n",
    "                            key, value = line.strip().split(': ')\n",
    "                            # Check if the key is 'client_learning_rate'\n",
    "                            if key == 'client_learning_rate':\n",
    "                                # Assign the value to the variable\n",
    "                                n = float(value)\n",
    "                                # Exit the loop as we found the value\n",
    "                            elif key == 'server_learning_rate':\n",
    "                                ns = float(value)\n",
    "                        \n",
    "            matching_files = [f for f in os.listdir(source_path) if pattern in f and 'validation_data.csv' in f]\n",
    "            for file_name in matching_files:\n",
    "                c1=get_avg_metrics(source_path+\"/\"+file_name, 'Accuracy', 'convergence', 5, th1) \n",
    "                c2=get_avg_metrics(source_path+\"/\"+file_name, 'Accuracy', 'convergence', 5, th2) \n",
    "                if c1 != None:\n",
    "                    c1*=5\n",
    "                if c2 != None:\n",
    "                    c2*=5\n",
    "                data = pd.read_csv(source_path+\"/\"+file_name)\n",
    "                # Extract the first 5000 points (adjust as needed)\n",
    "                selected_data = data.iloc[:total_rounds]\n",
    "                # Calculate the average of the last 500 points\n",
    "                last_500_average = selected_data['Accuracy']\n",
    "                last_500_average = last_500_average.apply(extract_float_value).tail(100).mean()        \n",
    "                # Print or use the calculated average as needed\n",
    "                #max_acc=str(last_500_average).replace('.', ',')\n",
    "                max_acc=str(last_500_average)\n",
    "                print(f\"{algorithm},{str(e)},{str(c)},{str(ns)},{str(n)},{str(c1)},{str(c2)},{max_acc}\")\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df727e-f2a8-45bd-94f8-470e965c6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def extract_tensor_value(tensor_str):\n",
    "    \"\"\"\n",
    "    Extracts the numeric value from a string in the format:\n",
    "    \"tf.Tensor(42.425056, shape=(), dtype=float32)\"\n",
    "    \"\"\"\n",
    "    match = re.search(r\"tf\\.Tensor\\(([-+]?[0-9]*\\.?[0-9]+)\", tensor_str)\n",
    "    return float(match.group(1)) if match else np.nan\n",
    "\n",
    "# Define the base directory for the results\n",
    "# (update path as needed)\n",
    "base_dir = 'results/official/cifar100'\n",
    "algorithms = ['fedAvg', 'fedAdam', 'fedAdadb']\n",
    "num_runs = 5  # Number of runs per algorithm\n",
    "\n",
    "# Initialize dictionary to collect final-accuracy results\n",
    "metric_data = {'Algorithm': [], 'FinalAccuracy': []}\n",
    "\n",
    "# Iterate through each algorithm and its runs\n",
    "for algorithm in algorithms:\n",
    "    runs_dir = os.path.join(base_dir, algorithm, 'training', 'runs')\n",
    "    for run_number in range(1, num_runs + 1):\n",
    "        file_path = os.path.join(runs_dir, str(run_number), 'c10_e4_validation_data.csv')\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure numeric accuracy\n",
    "            if data['Accuracy'].dtype == object:\n",
    "                data['Accuracy'] = data['Accuracy'].apply(extract_tensor_value)\n",
    "\n",
    "            # Filter to final 100 rounds (1901-2000)\n",
    "            final_rounds = data[(data['Round'] >= 1901) & (data['Round'] <= 2000)]\n",
    "\n",
    "            if not final_rounds.empty:\n",
    "                final_acc = final_rounds['Accuracy'].mean()\n",
    "                metric_data['Algorithm'].append(algorithm)\n",
    "                metric_data['FinalAccuracy'].append(final_acc)\n",
    "            else:\n",
    "                print(f\"Insufficient final rounds in {file_path}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File missing: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Build DataFrame\n",
    "df_metric = pd.DataFrame(metric_data)\n",
    "print(\"\\nFinal Model Quality (Avg. Last 100 Rounds):\")\n",
    "print(df_metric)\n",
    "\n",
    "# --- Statistical Testing (Keep as is) ---\n",
    "# Prepare for Friedman test\n",
    "groups = [\n",
    "    df_metric[df_metric['Algorithm'] == alg]['FinalAccuracy'].values[:num_runs]\n",
    "    for alg in algorithms\n",
    "]\n",
    "\n",
    "# Run Friedman test if possible\n",
    "if all(len(g) >= 2 for g in groups):\n",
    "    # Ensure groups have the same length for Friedman test\n",
    "    min_len = min(len(g) for g in groups)\n",
    "    if min_len < num_runs:\n",
    "        print(f\"Warning: Not all algorithms have {num_runs} runs. Using {min_len} runs for testing.\")\n",
    "        groups = [g[:min_len] for g in groups]\n",
    "\n",
    "    if min_len >= 2: # Check again after potential truncation\n",
    "        stat, p = friedmanchisquare(*groups)\n",
    "        print(f\"\\nFriedman stat: {stat:.3f}, p-value: {p:.3e}\")\n",
    "        if p < 0.05:\n",
    "            print(\"Significant: running Nemenyi post-hoc\")\n",
    "            # Prepare matrix for posthoc test correctly after potential truncation\n",
    "            matrix = np.vstack([g[:min_len] for g in groups]).T # Use min_len\n",
    "            p_nem = sp.posthoc_nemenyi_friedman(matrix)\n",
    "            p_nem.index = p_nem.columns = algorithms\n",
    "            print(\"\\nNemenyi pairwise p-values:\")\n",
    "            print(p_nem)\n",
    "        else:\n",
    "            print(\"No significant differences.\")\n",
    "    else:\n",
    "        print(\"Need at least 2 runs per algorithm (with data) for statistical testing.\")\n",
    "else:\n",
    "    print(\"Need at least 2 runs per algorithm for statistical testing.\")\n",
    "# --- End Statistical Testing ---\n",
    "\n",
    "\n",
    "# --- Plotting and Saving ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Algorithm', y='FinalAccuracy', data=df_metric)\n",
    "plt.title('Boxplot Comparison of Final Validation Accuracy (Rounds 1901–2000)')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "# Define filename and parameters for saving\n",
    "# You can choose PNG (raster) or PDF (vector, often better for papers)\n",
    "output_filename_png = 'boxplot_final_accuracy.png'\n",
    "output_filename_pdf = 'boxplot_final_accuracy.pdf'\n",
    "dpi_setting = 300 # Common requirement for papers, increase to 600 if needed\n",
    "\n",
    "# Save the plot BEFORE showing it\n",
    "# Save as PNG\n",
    "plt.savefig(output_filename_png, dpi=dpi_setting, bbox_inches='tight')\n",
    "print(f\"\\nPlot saved as high-quality PNG: {output_filename_png}\")\n",
    "\n",
    "# Save as PDF (vector format is great for scaling in papers)\n",
    "plt.savefig(output_filename_pdf, bbox_inches='tight') # DPI is less relevant for vector formats like PDF\n",
    "print(f\"Plot saved as high-quality PDF: {output_filename_pdf}\")\n",
    "\n",
    "# Now display the plot (optional)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bc8ae-31c9-4492-a379-d04a4afc9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# --- Tensor extractor ---\n",
    "def extract_tensor_value(tensor_str):\n",
    "    match = re.search(r\"tf\\.Tensor\\(([-+]?[0-9]*\\.?[0-9]+)\", tensor_str)\n",
    "    return float(match.group(1)) if match else np.nan\n",
    "\n",
    "# --- Configuration ---\n",
    "base_dir = 'results/official/cifar100'\n",
    "algorithms = ['fedAvg', 'fedAdam', 'fedAdadb']\n",
    "num_runs = 5\n",
    "\n",
    "# --- Load data ---\n",
    "metric_data = {'Algorithm': [], 'FinalAccuracy': [], 'Run': []}\n",
    "for algorithm in algorithms:\n",
    "    for run_number in range(1, num_runs + 1):\n",
    "        path = os.path.join(base_dir, algorithm, 'training', 'runs', str(run_number), 'c10_e4_validation_data.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            if df['Accuracy'].dtype == object:\n",
    "                df['Accuracy'] = df['Accuracy'].apply(extract_tensor_value)\n",
    "            final_acc = df[(df['Round'] >= 1901) & (df['Round'] <= 2000)]['Accuracy'].mean()\n",
    "            metric_data['Algorithm'].append(algorithm)\n",
    "            metric_data['FinalAccuracy'].append(final_acc)\n",
    "            metric_data['Run'].append(run_number)\n",
    "        except:\n",
    "            print(f\"Missing or failed file: {path}\")\n",
    "\n",
    "df = pd.DataFrame(metric_data)\n",
    "pivot_df = df.pivot(index='Run', columns='Algorithm', values='FinalAccuracy')\n",
    "\n",
    "# --- Paired t-tests ---\n",
    "print(\"Paired t-tests:\")\n",
    "for a1, a2 in [('fedAdadb', 'fedAdam'), ('fedAdadb', 'fedAvg')]:\n",
    "    t_stat, p_val = ttest_rel(pivot_df[a1], pivot_df[a2])\n",
    "    print(f\"{a1} vs {a2}: t = {t_stat:.3f}, p = {p_val:.3e}\")\n",
    "\n",
    "# --- Boxplot of final accuracies ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Algorithm', y='FinalAccuracy', data=df)\n",
    "plt.title('Final Accuracy (Avg. of Last 100 Rounds)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('boxplot_accuracy.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- Histogram of Differences ---\n",
    "diffs = {\n",
    "    'fedAdadb vs fedAdam': pivot_df['fedAdadb'] - pivot_df['fedAdam'],\n",
    "    'fedAdadb vs fedAvg': pivot_df['fedAdadb'] - pivot_df['fedAvg'],\n",
    "}\n",
    "\n",
    "for label, values in diffs.items():\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(values, kde=True)\n",
    "    plt.title(f'Difference in Accuracy: {label}')\n",
    "    plt.xlabel('Accuracy Difference')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'diff_{label.replace(\" \", \"_\").replace(\"vs\", \"vs_\")}.png', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a48a2a-9ede-4687-8456-63df60a19088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Regular expression pattern to extract float values\n",
    "float_pattern = r'tf\\.Tensor\\(([-+]?\\d*\\.\\d+|\\d+),.*\\)'\n",
    "\n",
    "# Function to extract float value from a tensor or use float directly\n",
    "def extract_float_value(value):\n",
    "    \"\"\"Extracts a float value from a string potentially representing a TensorFlow Tensor.\"\"\"\n",
    "    # Convert value to string to handle potential non-string inputs safely\n",
    "    value_str = str(value)\n",
    "    match = re.match(float_pattern, value_str)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except (ValueError, TypeError):\n",
    "            # Handle cases where the extracted group is not a valid float\n",
    "            print(f\"Warning: Could not convert extracted value '{match.group(1)}' to float. Original value: {value_str}\")\n",
    "            return np.nan # Return NaN or handle appropriately\n",
    "    else:\n",
    "        try:\n",
    "            # Attempt to convert directly if it doesn't match the tensor pattern\n",
    "            return float(value_str)\n",
    "        except (ValueError, TypeError):\n",
    "             # Handle cases where the value is not a float or tensor string\n",
    "            print(f\"Warning: Could not convert value '{value_str}' to float.\")\n",
    "            return np.nan # Return NaN or handle appropriately\n",
    "\n",
    "def plot_multiple_runs(base_directory, run_numbers, csv_filename, column_name, window_size, start_round, end_round, rounds_per_eval, output_filename):\n",
    "    \"\"\"\n",
    "    Plots data from a specific column of CSV files located in different run directories.\n",
    "\n",
    "    Args:\n",
    "        base_directory (str): The base path containing the numbered 'run' subdirectories.\n",
    "        run_numbers (list): A list of integers representing the run directories to plot.\n",
    "        csv_filename (str): The name of the CSV file within each run directory.\n",
    "        column_name (str): The name of the column to plot from the CSV files.\n",
    "        window_size (int): The window size for the moving average. Use 1 for no smoothing.\n",
    "        start_round (int): The starting round index for plotting.\n",
    "        end_round (int): The ending round index for plotting.\n",
    "        rounds_per_eval (int): The number of rounds between each evaluation point.\n",
    "        output_filename (str): The path and filename to save the plot.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 7)) # Increased figure size slightly\n",
    "\n",
    "    all_data_found = True # Flag to track if all files were found\n",
    "\n",
    "    for run_num in run_numbers:\n",
    "        # Construct the full path to the CSV file for the current run\n",
    "        file_path = os.path.join(base_directory, str(run_num), csv_filename)\n",
    "\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            # Use error_bad_lines=False if pandas version < 1.4.0, otherwise on_bad_lines='skip'\n",
    "            # Adjust nrows if you know the maximum expected rows to optimize loading\n",
    "            data = pd.read_csv(file_path) # Consider adding on_bad_lines='skip' for robustness\n",
    "\n",
    "            # Extract the specified column\n",
    "            if column_name not in data.columns:\n",
    "                 print(f\"Warning: Column '{column_name}' not found in {file_path}. Skipping this run.\")\n",
    "                 continue # Skip to the next run if column is missing\n",
    "\n",
    "            data_column = data[column_name].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "            # Apply the function to extract float values, handling potential NaNs\n",
    "            data_column = data_column.apply(extract_float_value)\n",
    "            data_column = data_column.dropna() # Remove rows where float extraction failed\n",
    "\n",
    "            if data_column.empty:\n",
    "                print(f\"Warning: No valid data found in column '{column_name}' for {file_path} after processing. Skipping this run.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate x-axis values (Rounds)\n",
    "            num_points = len(data_column)\n",
    "            x_values = np.arange(num_points) * rounds_per_eval\n",
    "\n",
    "            # Calculate moving average if window_size > 1\n",
    "            if window_size > 1:\n",
    "                # Calculate moving average, handling the initial window\n",
    "                moving_average = data_column.rolling(window=window_size, min_periods=1).mean()\n",
    "                plot_data = moving_average\n",
    "            else:\n",
    "                # Plot raw data if window_size is 1 or less\n",
    "                plot_data = data_column\n",
    "\n",
    "            # Determine indices for slicing based on rounds\n",
    "            start_index = max(0, start_round // rounds_per_eval)\n",
    "            # Ensure end_index doesn't exceed the number of points\n",
    "            end_index = min(num_points, (end_round + rounds_per_eval -1) // rounds_per_eval ) # + rounds_per_eval -1 ensures inclusion\n",
    "\n",
    "\n",
    "            # Plot the data for the specified range\n",
    "            ax.plot(x_values[start_index:end_index],\n",
    "                    plot_data.iloc[start_index:end_index],\n",
    "                    label=f'Run {run_num}', alpha=0.8) # Use iloc for positional indexing\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found at {file_path}. Skipping this run.\")\n",
    "            all_data_found = False\n",
    "        except pd.errors.EmptyDataError:\n",
    "             print(f\"Warning: File is empty at {file_path}. Skipping this run.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "            all_data_found = False\n",
    "\n",
    "\n",
    "    # Add plot labels, title, and legend\n",
    "    ax.set_xlabel('Round')\n",
    "    ax.set_ylabel(column_name)\n",
    "    ax.set_title(f'Validation {column_name} Across Runs')\n",
    "    ax.legend(loc='best') # Changed to 'best' for potentially better placement\n",
    "    ax.grid(True, linestyle='--', alpha=0.6) # Added grid for readability\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    try:\n",
    "        plt.savefig(output_filename)\n",
    "        print(f\"Plot saved to {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot: {e}\")\n",
    "\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    if not all_data_found:\n",
    "        print(\"Note: Some run data files were not found or could not be processed.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "base_dir = \"results/official/shakespeare/fedAdadb/training/runs/\"\n",
    "run_ids = [1, 2, 3, 4, 5]\n",
    "csv_file = \"c10_e4_validation_data_new.csv\"\n",
    "col_to_plot = 'Accuracy'\n",
    "smoothing_window = 1  # Set to 1 to plot raw data points, >1 for moving average\n",
    "plot_start_round = 0   # Start plotting from this round\n",
    "plot_end_round = 2000  # Stop plotting at this round (inclusive if data exists)\n",
    "eval_frequency = 5     # Rounds per evaluation point in the CSV\n",
    "output_plot_file = \"figures/shakespeare_fedAvg_runs_accuracy.jpg\"\n",
    "\n",
    "# --- Create output directory if it doesn't exist ---\n",
    "output_dir = os.path.dirname(output_plot_file)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "\n",
    "# --- Generate the plot ---\n",
    "plot_multiple_runs(\n",
    "    base_directory=base_dir,\n",
    "    run_numbers=run_ids,\n",
    "    csv_filename=csv_file,\n",
    "    column_name=col_to_plot,\n",
    "    window_size=smoothing_window,\n",
    "    start_round=plot_start_round,\n",
    "    end_round=plot_end_round,\n",
    "    rounds_per_eval=eval_frequency,\n",
    "    output_filename=output_plot_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a05a6e5-63f7-494a-aa03-e1ca2d44bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "\n",
    "# Regular expression pattern to extract float values (consistent with your original)\n",
    "float_pattern = r'tf\\.Tensor\\(([-+]?\\d*\\.\\d+|\\d+),.*\\)'\n",
    "\n",
    "# Function to extract float value from a tensor or use float directly (consistent with your original)\n",
    "def extract_float_value(value):\n",
    "    \"\"\"Extracts a float value from a string potentially representing a TensorFlow Tensor.\"\"\"\n",
    "    # Convert value to string to handle potential non-string inputs safely\n",
    "    value_str = str(value)\n",
    "    match = re.match(float_pattern, value_str)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except (ValueError, TypeError):\n",
    "            # Handle cases where the extracted group is not a valid float\n",
    "            print(f\"Warning: Could not convert extracted value '{match.group(1)}' to float. Original value: {value_str}\")\n",
    "            return np.nan # Return NaN or handle appropriately\n",
    "    else:\n",
    "        try:\n",
    "            # Attempt to convert directly if it doesn't match the tensor pattern\n",
    "            return float(value_str)\n",
    "        except (ValueError, TypeError):\n",
    "             # Handle cases where the value is not a float or tensor string\n",
    "            # print(f\"Warning: Could not convert value '{value_str}' to float.\") # Optional: Reduce verbosity\n",
    "            return np.nan # Return NaN or handle appropriately\n",
    "\n",
    "def plot_algorithm_comparison(\n",
    "    datasets: List[str],\n",
    "    algorithms: List[str],\n",
    "    run_numbers: List[int],\n",
    "    base_results_dir: str = \"results/official\",\n",
    "    csv_filename_template: str = \"c10_e4_validation_data_new.csv\", # Use a template or ensure consistency\n",
    "    column_name: str = 'Accuracy',\n",
    "    window_size: int = 1,\n",
    "    start_round: int = 0,\n",
    "    end_round: Optional[int] = None, # Allow plotting to the very end if None\n",
    "    rounds_per_eval: int = 5,\n",
    "    output_base_dir: str = \"figures/comparisons\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a comparison of multiple algorithms across multiple runs for specified datasets.\n",
    "\n",
    "    Shows mean performance with standard deviation shading.\n",
    "\n",
    "    Args:\n",
    "        datasets (List[str]): List of dataset names (e.g., [\"emnist\", \"shakespeare\"]).\n",
    "        algorithms (List[str]): List of algorithm names (e.g., [\"fedAvg\", \"fedAdam\"]).\n",
    "        run_numbers (List[int]): List of run numbers to aggregate for each algorithm.\n",
    "        base_results_dir (str): Base path containing dataset directories.\n",
    "        csv_filename_template (str): Filename of the CSV within each run directory.\n",
    "                                     Ensure this is consistent or adapt logic if needed.\n",
    "        column_name (str): The name of the column to plot.\n",
    "        window_size (int): Window size for moving average smoothing applied to the mean curve. Use 1 for no smoothing.\n",
    "        start_round (int): The starting communication round for plotting.\n",
    "        end_round (Optional[int]): The ending communication round for plotting. If None, plots all available data.\n",
    "        rounds_per_eval (int): Number of rounds between each evaluation point in the CSV.\n",
    "        output_base_dir (str): Base directory where comparison plots will be saved.\n",
    "    \"\"\"\n",
    "    # --- Create base output directory if it doesn't exist ---\n",
    "    if output_base_dir and not os.path.exists(output_base_dir):\n",
    "        os.makedirs(output_base_dir)\n",
    "        print(f\"Created base output directory: {output_base_dir}\")\n",
    "\n",
    "    # Loop through each dataset specified\n",
    "    for dataset in datasets:\n",
    "        print(f\"\\n--- Processing Dataset: {dataset} ---\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 8)) # Larger figure for clarity\n",
    "        results_found_for_dataset = False # Flag if any data is plotted for this dataset\n",
    "\n",
    "        # Define a color cycle for consistency across algorithms\n",
    "        # Using tab10 which is good for distinct colors\n",
    "        colors = plt.cm.tab10.colors[:len(algorithms)]\n",
    "\n",
    "        # Loop through each algorithm to plot\n",
    "        for i, algorithm in enumerate(algorithms):\n",
    "            print(f\" Processing Algorithm: {algorithm}\")\n",
    "            # Construct the base path for the specific algorithm and dataset\n",
    "            algo_base_path = os.path.join(base_results_dir, dataset, algorithm, \"training\", \"runs\")\n",
    "            run_data_list = []\n",
    "            max_points_algo = 0 # Track max data points for this algorithm's runs\n",
    "\n",
    "            # --- Aggregate data across runs for the current algorithm ---\n",
    "            for run_num in run_numbers:\n",
    "                # Assuming csv_filename is consistent, otherwise adapt path construction\n",
    "                file_path = os.path.join(algo_base_path, str(run_num), csv_filename_template)\n",
    "                try:\n",
    "                    # Use error handling for robust reading\n",
    "                    data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "                    if column_name not in data.columns:\n",
    "                        print(f\" Warning: Column '{column_name}' missing in {file_path}. Skipping run {run_num}.\")\n",
    "                        continue\n",
    "\n",
    "                    data_column = data[column_name].copy()\n",
    "                    data_column = data_column.apply(extract_float_value)\n",
    "                    # Drop rows where extraction failed or original value was NaN\n",
    "                    data_column = data_column.dropna()\n",
    "\n",
    "                    if data_column.empty:\n",
    "                        print(f\" Warning: No valid data in column '{column_name}' for {file_path}. Skipping run {run_num}.\")\n",
    "                        continue\n",
    "\n",
    "                    # Store the cleaned data series, reset index for easier alignment later\n",
    "                    run_data_list.append(data_column.reset_index(drop=True))\n",
    "                    max_points_algo = max(max_points_algo, len(data_column))\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    print(f\" Warning: File not found {file_path}. Skipping run {run_num}.\")\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\" Warning: File empty {file_path}. Skipping run {run_num}.\")\n",
    "                except Exception as e:\n",
    "                    print(f\" Error processing {file_path}: {e}. Skipping run {run_num}.\")\n",
    "\n",
    "            # Check if any valid data was collected for this algorithm\n",
    "            if not run_data_list:\n",
    "                print(f\" Error: No valid data found for any specified run of algorithm '{algorithm}'. Skipping this algorithm for dataset '{dataset}'.\")\n",
    "                continue # Skip to the next algorithm\n",
    "\n",
    "            # --- Data Alignment and Calculation ---\n",
    "            # Create a DataFrame with runs as columns, index as evaluation step\n",
    "            # Aligns series based on index, shorter runs will be padded with NaN\n",
    "            aligned_data = pd.concat(run_data_list, axis=1)\n",
    "            # Rename columns for clarity (optional)\n",
    "            aligned_data.columns = [f'run_{run_num}' for run_num in run_numbers[:len(aligned_data.columns)]] # Handle cases where some runs failed\n",
    "\n",
    "            # Calculate mean and std dev across runs (axis=1), skipping NaNs\n",
    "            mean_values = aligned_data.mean(axis=1, skipna=True)\n",
    "            std_values = aligned_data.std(axis=1, skipna=True)\n",
    "\n",
    "            # Apply moving average smoothing to the MEAN curve if window_size > 1\n",
    "            if window_size > 1:\n",
    "                # Use min_periods=1 to get rolling mean even at the beginning\n",
    "                plot_mean = mean_values.rolling(window=window_size, min_periods=1).mean()\n",
    "                # Standard deviation is typically NOT smoothed, or requires careful consideration\n",
    "                plot_std = std_values\n",
    "            else:\n",
    "                plot_mean = mean_values\n",
    "                plot_std = std_values\n",
    "\n",
    "            # --- Prepare data for plotting (slicing based on rounds) ---\n",
    "            num_points = len(plot_mean)\n",
    "            x_values = np.arange(num_points) * rounds_per_eval\n",
    "\n",
    "            # Determine indices for slicing based on rounds\n",
    "            start_index = 0\n",
    "            if start_round > 0:\n",
    "                 # Find the first index where round >= start_round\n",
    "                 start_indices = np.where(x_values >= start_round)[0]\n",
    "                 if len(start_indices) > 0:\n",
    "                     start_index = start_indices[0]\n",
    "\n",
    "            end_index = num_points # Default to end of data\n",
    "            if end_round is not None:\n",
    "                 # Find the first index where round > end_round\n",
    "                 end_indices = np.where(x_values > end_round)[0]\n",
    "                 if len(end_indices) > 0:\n",
    "                     end_index = end_indices[0] # Slice up to this index (exclusive)\n",
    "\n",
    "\n",
    "            # Slice data for the plot\n",
    "            x_plot = x_values[start_index:end_index]\n",
    "            mean_plot = plot_mean.iloc[start_index:end_index]\n",
    "            std_plot = plot_std.iloc[start_index:end_index] # Ensure std dev is sliced consistently\n",
    "\n",
    "            # Check if there's actually data to plot in the specified range\n",
    "            if len(x_plot) == 0:\n",
    "                 print(f\" Warning: No data points available for algorithm '{algorithm}' in the specified round range ({start_round}-{end_round}). Skipping plot segment.\")\n",
    "                 continue\n",
    "\n",
    "            # --- Plotting for the current algorithm ---\n",
    "            color = colors[i]\n",
    "            ax.plot(x_plot, mean_plot, label=f'{algorithm}', color=color, linewidth=2.5)\n",
    "            # Fill between mean +/- std deviation\n",
    "            ax.fill_between(x_plot,\n",
    "                            (mean_plot - std_plot).clip(lower=0), # Prevent std dev going below 0 visually\n",
    "                            mean_plot + std_plot,\n",
    "                            color=color, alpha=0.25) # Lighter alpha for shading\n",
    "            results_found_for_dataset = True # Mark that we plotted something\n",
    "\n",
    "        # --- Final Plot Configuration for the dataset ---\n",
    "        if not results_found_for_dataset:\n",
    "            print(f\"Error: No data could be plotted for dataset '{dataset}'. Skipping plot generation.\")\n",
    "            plt.close(fig) # Close the empty figure\n",
    "            continue\n",
    "\n",
    "        ax.set_xlabel('Communication Round', fontsize=14)\n",
    "        ax.set_ylabel(f'Validation {column_name}', fontsize=14)\n",
    "        ax.set_title(f'{dataset.upper()} Validation {column_name} Comparison', fontsize=16)\n",
    "        ax.legend(loc='best', fontsize=12)\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        # --- Adjust Y-axis Limits ---\n",
    "        # Get the automatic limits set by matplotlib after plotting all data\n",
    "        current_bottom, current_top = ax.get_ylim()\n",
    "\n",
    "        # Check if the column name suggests a percentage scale\n",
    "        is_percentage_metric = 'accuracy' in column_name.lower() or 'percent' in column_name.lower()\n",
    "\n",
    "        if is_percentage_metric:\n",
    "            print(f\"Adjusting y-axis for percentage scale (0-100) for metric: {column_name}\")\n",
    "            # For metrics scaled 0-100:\n",
    "            # Ensure bottom is at 0 (or slightly below if data dips negative due to noise/smoothing)\n",
    "            plot_bottom = max(0, current_bottom) if current_bottom < 10 else 0 # Handle potential noise making min slightly negative\n",
    "            plot_bottom = 0 # Usually best to fix bottom at 0 for percentage\n",
    "\n",
    "            # Ensure top is slightly above 100 or the max data point, whichever is higher\n",
    "            plot_top = max(101, current_top * 1.02) # Ensure at least 101, give 2% margin above max data point\n",
    "            plot_top = min(plot_top, 105) # Optionally cap the view slightly above 100, e.g. 105\n",
    "\n",
    "            ax.set_ylim(bottom=plot_bottom, top=plot_top)\n",
    "\n",
    "        elif 'loss' in column_name.lower():\n",
    "            # For loss metrics, ensure the bottom is at or very close to 0\n",
    "            print(f\"Adjusting y-axis for loss scale for metric: {column_name}\")\n",
    "            plot_bottom = max(0, current_bottom) if current_bottom < 1 else 0 # Allow slightly negative bottom only if necessary due to noise\n",
    "            ax.set_ylim(bottom=plot_bottom)\n",
    "            # Let the top auto-scale for loss, maybe add a small margin\n",
    "            # ax.set_ylim(bottom=plot_bottom, top=current_top * 1.05)\n",
    "\n",
    "        # else: For other metrics, we might keep the default auto-scaling\n",
    "        # print(f\"Using default auto-scaling for y-axis for metric: {column_name}\")\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        plt.tight_layout() # Adjust layout AFTER setting ylim\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        plt.tight_layout() # Adjust layout\n",
    "\n",
    "        # --- Save the plot ---\n",
    "        # Generate a descriptive filename\n",
    "        output_filename = os.path.join(output_base_dir, f\"{dataset}_algos_comparison_{column_name.lower().replace(' ', '_')}.jpg\")\n",
    "\n",
    "        try:\n",
    "            plt.savefig(output_filename, dpi=300, bbox_inches='tight') # Use tight bounding box\n",
    "            print(f\" Plot saved to {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error saving plot {output_filename}: {e}\")\n",
    "\n",
    "        plt.show() # Display the plot for the current dataset\n",
    "        plt.close(fig) # Close figure before processing next dataset\n",
    "\n",
    "# --- Configuration ---\n",
    "datasets_to_plot = [\"emnist\", \"shakespeare\", \"cifar100\"] # Select datasets\n",
    "# datasets_to_plot = [\"emnist\"] # Example: Just plot for EMNIST\n",
    "\n",
    "algorithms_to_plot = [\"fedAvg\", \"fedAdam\", \"fedAdadb\"] # Select algorithms\n",
    "run_ids_to_process = [1, 2, 3, 4, 5] # Specify run numbers to average over\n",
    "\n",
    "base_results = \"results/official\" # Path containing dataset folders\n",
    "\n",
    "# *** IMPORTANT: Verify this CSV filename exists and is correct for ALL datasets/algorithms ***\n",
    "# If filenames differ, you'll need more complex logic to determine the filename per dataset/algo\n",
    "common_csv_file = \"c10_e4_validation_data.csv\"\n",
    "col_to_plot = 'Accuracy'\n",
    "\n",
    "smoothing_window = 5      # Window size for smoothing the MEAN curve (e.g., 5). 1 = no smoothing.\n",
    "plot_start_round = 0      # Start plotting from this communication round\n",
    "plot_end_round = 2000     # Stop plotting at this communication round (set to None to plot all data)\n",
    "eval_frequency = 5        # IMPORTANT: Ensure this matches the frequency used during experiments\n",
    "\n",
    "output_plot_dir = \"figures/comparisons\" # Directory to save comparison plots\n",
    "\n",
    "# --- Generate the plots ---\n",
    "plot_algorithm_comparison(\n",
    "    datasets=datasets_to_plot,\n",
    "    algorithms=algorithms_to_plot,\n",
    "    run_numbers=run_ids_to_process,\n",
    "    base_results_dir=base_results,\n",
    "    csv_filename_template=common_csv_file, # Pass template name\n",
    "    column_name=col_to_plot,\n",
    "    window_size=smoothing_window,\n",
    "    start_round=plot_start_round,\n",
    "    end_round=plot_end_round,\n",
    "    rounds_per_eval=eval_frequency,\n",
    "    output_base_dir=output_plot_dir\n",
    ")\n",
    "\n",
    "print(\"\\n--- Plotting script finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46da26-4de0-414d-ae44-cd6c19bf25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# (Keep extract_float_value function as is)\n",
    "# ... (extract_float_value function from previous code) ...\n",
    "def extract_float_value(value):\n",
    "    \"\"\"Extracts a float value from a string potentially representing a TensorFlow Tensor.\"\"\"\n",
    "    # Convert value to string to handle potential non-string inputs safely\n",
    "    value_str = str(value)\n",
    "    match = re.match(float_pattern, value_str)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except (ValueError, TypeError):\n",
    "            # Handle cases where the extracted group is not a valid float\n",
    "            print(f\"Warning: Could not convert extracted value '{match.group(1)}' to float. Original value: {value_str}\")\n",
    "            return np.nan # Return NaN or handle appropriately\n",
    "    else:\n",
    "        try:\n",
    "            # Attempt to convert directly if it doesn't match the tensor pattern\n",
    "            return float(value_str)\n",
    "        except (ValueError, TypeError):\n",
    "             # Handle cases where the value is not a float or tensor string\n",
    "            # print(f\"Warning: Could not convert value '{value_str}' to float.\") # Optional: Reduce verbosity\n",
    "            return np.nan # Return NaN or handle appropriately\n",
    "\n",
    "def calculate_metrics(\n",
    "    mean_values: pd.Series,\n",
    "    x_values: np.ndarray,\n",
    "    thresholds_to_check: List[float] = [75.0, 85.0],\n",
    "    consistency_window: int = 5 # Number of consecutive points (including current) >= threshold\n",
    ") -> Dict[float, float]:\n",
    "    \"\"\"Calculates the rounds required to consistently reach specified thresholds.\"\"\"\n",
    "    rounds_achieved = {thr: np.inf for thr in thresholds_to_check}\n",
    "\n",
    "    if mean_values.empty:\n",
    "        return rounds_achieved\n",
    "\n",
    "    # Use rolling window to check for consistency\n",
    "    rolling_min = mean_values.rolling(window=consistency_window, min_periods=consistency_window).min()\n",
    "\n",
    "    for threshold in thresholds_to_check:\n",
    "        # Find first index where the rolling minimum meets or exceeds the threshold\n",
    "        # This ensures 'consistency_window' consecutive points were >= threshold\n",
    "        indices = np.where(rolling_min >= threshold)[0]\n",
    "\n",
    "        if len(indices) > 0:\n",
    "            first_consistent_index = indices[0]\n",
    "            # The round corresponds to the *end* of the first consistent window\n",
    "            if first_consistent_index < len(x_values):\n",
    "                 rounds_achieved[threshold] = x_values[first_consistent_index]\n",
    "\n",
    "    return rounds_achieved\n",
    "\n",
    "\n",
    "def plot_algorithm_comparison(\n",
    "    datasets: List[str],\n",
    "    algorithms: List[str],\n",
    "    run_numbers: List[int],\n",
    "    base_results_dir: str = \"results/official\",\n",
    "    csv_filename_template: str = \"c10_e4_validation_data_new.csv\",\n",
    "    column_name: str = 'Accuracy',\n",
    "    window_size: int = 1,\n",
    "    start_round: int = 0,\n",
    "    end_round: Optional[int] = None,\n",
    "    rounds_per_eval: int = 5,\n",
    "    output_base_dir: str = \"figures/comparisons\",\n",
    "    metric_thresholds: List[float] = [75.0, 85.0], # Thresholds for Metric 2\n",
    "    post_avg_threshold: float = 85.0, # Threshold for Metric 3\n",
    "    metric_consistency_window: int = 5 # Window for Metric 2 consistency check\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots algorithm comparison, calculates, and prints performance metrics.\n",
    "    (Args documentation mostly unchanged, added metric args)\n",
    "    \"\"\"\n",
    "    # --- Create base output directory ---\n",
    "    if output_base_dir and not os.path.exists(output_base_dir):\n",
    "        os.makedirs(output_base_dir)\n",
    "        print(f\"Created base output directory: {output_base_dir}\")\n",
    "\n",
    "    all_datasets_metrics = {} # Store metrics for all datasets\n",
    "\n",
    "    # Loop through each dataset\n",
    "    for dataset in datasets:\n",
    "        print(f\"\\n--- Processing Dataset: {dataset} ---\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        results_found_for_dataset = False\n",
    "        colors = plt.cm.tab10.colors[:len(algorithms)]\n",
    "\n",
    "        # --- Data storage for metrics calculation ---\n",
    "        metrics_for_current_dataset: Dict[str, Dict[str, Any]] = {}\n",
    "        mean_data_per_algo: Dict[str, pd.Series] = {}\n",
    "        x_values_per_algo: Dict[str, np.ndarray] = {}\n",
    "        max_round_in_dataset = 0 # Track the max round based on data length\n",
    "\n",
    "        # Loop through each algorithm\n",
    "        for i, algorithm in enumerate(algorithms):\n",
    "            print(f\" Processing Algorithm: {algorithm}\")\n",
    "            algo_base_path = os.path.join(base_results_dir, dataset, algorithm, \"training\", \"runs\")\n",
    "            run_data_list = []\n",
    "            # ... (Data aggregation loop - same as before) ...\n",
    "            for run_num in run_numbers:\n",
    "                # Assuming csv_filename is consistent, otherwise adapt path construction\n",
    "                file_path = os.path.join(algo_base_path, str(run_num), csv_filename_template)\n",
    "                try:\n",
    "                    # Use error handling for robust reading\n",
    "                    data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "                    if column_name not in data.columns:\n",
    "                        print(f\" Warning: Column '{column_name}' missing in {file_path}. Skipping run {run_num}.\")\n",
    "                        continue\n",
    "\n",
    "                    data_column = data[column_name].copy()\n",
    "                    data_column = data_column.apply(extract_float_value)\n",
    "                    # Drop rows where extraction failed or original value was NaN\n",
    "                    data_column = data_column.dropna()\n",
    "\n",
    "                    if data_column.empty:\n",
    "                        print(f\" Warning: No valid data in column '{column_name}' for {file_path}. Skipping run {run_num}.\")\n",
    "                        continue\n",
    "\n",
    "                    # Store the cleaned data series, reset index for easier alignment later\n",
    "                    run_data_list.append(data_column.reset_index(drop=True))\n",
    "                    #max_points_algo = max(max_points_algo, len(data_column)) # Not needed with concat\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    print(f\" Warning: File not found {file_path}. Skipping run {run_num}.\")\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\" Warning: File empty {file_path}. Skipping run {run_num}.\")\n",
    "                except Exception as e:\n",
    "                    print(f\" Error processing {file_path}: {e}. Skipping run {run_num}.\")\n",
    "\n",
    "\n",
    "            if not run_data_list:\n",
    "                print(f\" Error: No valid data found for any specified run of algorithm '{algorithm}'. Skipping this algorithm for dataset '{dataset}'.\")\n",
    "                metrics_for_current_dataset[algorithm] = { # Initialize metrics entry even if skipped\n",
    "                    'rounds_to_threshold': {thr: np.inf for thr in metric_thresholds},\n",
    "                    'post_threshold_avg_acc': np.nan\n",
    "                 }\n",
    "                continue\n",
    "\n",
    "            # --- Data Alignment and Calculation ---\n",
    "            aligned_data = pd.concat(run_data_list, axis=1)\n",
    "            aligned_data.columns = [f'run_{r}' for r in run_numbers[:len(aligned_data.columns)]]\n",
    "\n",
    "            # Calculate mean and std dev (used for plotting AND metrics)\n",
    "            # Use mean_values *before* smoothing for metric calculations\n",
    "            mean_values = aligned_data.mean(axis=1, skipna=True)\n",
    "            std_values = aligned_data.std(axis=1, skipna=True)\n",
    "\n",
    "            # Calculate x-axis values (Rounds)\n",
    "            num_points = len(mean_values)\n",
    "            x_values = np.arange(num_points) * rounds_per_eval\n",
    "            if len(x_values) > 0 :\n",
    "                 max_round_in_dataset = max(max_round_in_dataset, x_values[-1])\n",
    "\n",
    "            # Store data needed for cross-algorithm metrics\n",
    "            mean_data_per_algo[algorithm] = mean_values\n",
    "            x_values_per_algo[algorithm] = x_values\n",
    "\n",
    "            # --- Calculate Metric 2: Rounds to Threshold ---\n",
    "            rounds_achieved = calculate_metrics(\n",
    "                mean_values, x_values, metric_thresholds, metric_consistency_window\n",
    "            )\n",
    "            metrics_for_current_dataset[algorithm] = {\n",
    "                'rounds_to_threshold': rounds_achieved,\n",
    "                'post_threshold_avg_acc': np.nan # Placeholder for Metric 3\n",
    "            }\n",
    "\n",
    "            # --- Smoothing for Plotting ---\n",
    "            if window_size > 1:\n",
    "                plot_mean = mean_values.rolling(window=window_size, min_periods=1).mean()\n",
    "                plot_std = std_values # Std dev not smoothed\n",
    "            else:\n",
    "                plot_mean = mean_values\n",
    "                plot_std = std_values\n",
    "\n",
    "            # --- Slicing for Plotting ---\n",
    "            # (Slicing logic - same as before) ...\n",
    "            start_index = 0\n",
    "            if start_round > 0:\n",
    "                 start_indices = np.where(x_values >= start_round)[0]\n",
    "                 if len(start_indices) > 0: start_index = start_indices[0]\n",
    "            effective_end_round = end_round if end_round is not None else max_round_in_dataset\n",
    "            end_index = num_points\n",
    "            if end_round is not None:\n",
    "                 end_indices = np.where(x_values > effective_end_round)[0]\n",
    "                 if len(end_indices) > 0: end_index = end_indices[0]\n",
    "\n",
    "            x_plot = x_values[start_index:end_index]\n",
    "            mean_plot = plot_mean.iloc[start_index:end_index]\n",
    "            std_plot = plot_std.iloc[start_index:end_index]\n",
    "\n",
    "            if len(x_plot) == 0:\n",
    "                 print(f\" Warning: No data points available for algorithm '{algorithm}' in the plot range. Skipping plot segment.\")\n",
    "                 continue\n",
    "\n",
    "            # --- Plotting ---\n",
    "            # (Plotting code - same as before) ...\n",
    "            color = colors[i]\n",
    "            ax.plot(x_plot, mean_plot, label=f'{algorithm}', color=color, linewidth=2.5)\n",
    "            ax.fill_between(x_plot,\n",
    "                            (mean_plot - std_plot).clip(lower=0), # Prevent std dev going below 0 visually\n",
    "                            mean_plot + std_plot,\n",
    "                            color=color, alpha=0.15) # Lighter alpha for shading\n",
    "            results_found_for_dataset = True # Mark that we plotted something\n",
    "\n",
    "\n",
    "        # --- Calculate Metric 3: Post-Threshold Average Accuracy ---\n",
    "        # This must be done after looping through all algorithms for the dataset\n",
    "        last_algo_reach_round = 0\n",
    "        all_algos_reached_metric3_thr = True\n",
    "        algos_reaching_thr = []\n",
    "\n",
    "        for algo, metrics in metrics_for_current_dataset.items():\n",
    "             if algo not in mean_data_per_algo: # Skip if algo had no data\n",
    "                 all_algos_reached_metric3_thr = False\n",
    "                 continue # Skip algos that failed entirely\n",
    "\n",
    "             round_reached = metrics['rounds_to_threshold'].get(post_avg_threshold, np.inf)\n",
    "             if round_reached == np.inf:\n",
    "                 all_algos_reached_metric3_thr = False\n",
    "                 # Decide: Calculate Metric 3 only for those who reached? Or only if ALL reached?\n",
    "                 # Let's calculate only if ALL algorithms *that had data* reached the threshold.\n",
    "                 print(f\" Info: Algorithm '{algo}' did not reach {post_avg_threshold}% threshold for Metric 3 calculation.\")\n",
    "                 # Break? Or continue to find max round among those who did?\n",
    "                 # If we need ALL algos, we set flag and might skip calculation later.\n",
    "             else:\n",
    "                 last_algo_reach_round = max(last_algo_reach_round, round_reached)\n",
    "                 algos_reaching_thr.append(algo)\n",
    "\n",
    "        if not all_algos_reached_metric3_thr or not algos_reaching_thr:\n",
    "             print(f\" Info: Metric 3 (Post-{post_avg_threshold}% Avg Acc) cannot be calculated because not all algorithms reached the threshold.\")\n",
    "        else:\n",
    "             print(f\" Info: Calculating Metric 3 starting after round {last_algo_reach_round} (last algo hit {post_avg_threshold}%)\")\n",
    "             # Find start index for metric calculation (round *after* the last one reached)\n",
    "             # Need a reference x_values, assume they are similar length or use longest? Use per-algo.\n",
    "             for algo in algos_reaching_thr:\n",
    "                  mean_data = mean_data_per_algo[algo]\n",
    "                  x_vals = x_values_per_algo[algo]\n",
    "                  metric3_start_indices = np.where(x_vals > last_algo_reach_round)[0]\n",
    "\n",
    "                  if len(metric3_start_indices) > 0:\n",
    "                      metric3_start_index = metric3_start_indices[0]\n",
    "                      # Slice the mean data from this index onwards\n",
    "                      post_threshold_data = mean_data.iloc[metric3_start_index:]\n",
    "                      if not post_threshold_data.empty:\n",
    "                          avg_acc = post_threshold_data.mean(skipna=True)\n",
    "                          metrics_for_current_dataset[algo]['post_threshold_avg_acc'] = avg_acc\n",
    "                      else:\n",
    "                           metrics_for_current_dataset[algo]['post_threshold_avg_acc'] = np.nan # No data after start round\n",
    "                  else: # Reached threshold but no data points exist after last_algo_reach_round\n",
    "                      metrics_for_current_dataset[algo]['post_threshold_avg_acc'] = np.nan\n",
    "\n",
    "\n",
    "        # --- Final Plot Configuration ---\n",
    "        # (Plot labels, title, grid - same as before) ...\n",
    "        ax.set_xlabel('Communication Round', fontsize=14)\n",
    "        ax.set_ylabel(f'Validation {column_name}', fontsize=14)\n",
    "        ax.set_title(f'{dataset.upper()} Validation {column_name} Comparison', fontsize=16)\n",
    "        ax.legend(loc='best', fontsize=12)\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        # (Y-axis limit adjustments - same as before) ...\n",
    "        current_bottom, current_top = ax.get_ylim() # Get limits after plotting all data\n",
    "        is_percentage_metric = 'accuracy' in column_name.lower() or 'percent' in column_name.lower()\n",
    "        if is_percentage_metric:\n",
    "            #print(f\"Adjusting y-axis for percentage scale (0-100) for metric: {column_name}\")\n",
    "            plot_bottom = 0\n",
    "            plot_top = min(105, max(101, current_top * 1.02))\n",
    "            ax.set_ylim(bottom=plot_bottom, top=plot_top)\n",
    "        elif 'loss' in column_name.lower():\n",
    "            #print(f\"Adjusting y-axis for loss scale for metric: {column_name}\")\n",
    "            plot_bottom = max(0, current_bottom) if current_bottom < 1 else 0\n",
    "            ax.set_ylim(bottom=plot_bottom)\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # --- Save Plot ---\n",
    "        # (Saving code - same as before) ...\n",
    "        if results_found_for_dataset: # Only save if something was plotted\n",
    "             output_filename = os.path.join(output_base_dir, f\"{dataset}_algos_comparison_{column_name.lower().replace(' ', '_')}.jpg\")\n",
    "             try:\n",
    "                 plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "                 print(f\" Plot saved to {output_filename}\")\n",
    "             except Exception as e:\n",
    "                 print(f\" Error saving plot {output_filename}: {e}\")\n",
    "        else:\n",
    "             print(f\"Error: No data could be plotted for dataset '{dataset}'. Skipping plot generation.\")\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "        # --- Print Calculated Metrics ---\n",
    "        print(f\"\\n--- Calculated Metrics for Dataset: {dataset} ---\")\n",
    "        # Use the collected metrics_for_current_dataset dictionary\n",
    "        metrics_df = pd.DataFrame.from_dict(metrics_for_current_dataset, orient='index')\n",
    "\n",
    "        # Create columns for display\n",
    "        display_cols = []\n",
    "        for thr in metric_thresholds:\n",
    "             col_name = f'Rounds to {thr}%'\n",
    "             metrics_df[col_name] = metrics_df['rounds_to_threshold'].apply(lambda x: x.get(thr, np.inf))\n",
    "             display_cols.append(col_name)\n",
    "\n",
    "        metric3_col_name = f'Avg Acc Post-{post_avg_threshold}%'\n",
    "        metrics_df[metric3_col_name] = metrics_df['post_threshold_avg_acc']\n",
    "        display_cols.append(metric3_col_name)\n",
    "\n",
    "        # Format for printing\n",
    "        display_df = metrics_df[display_cols].copy()\n",
    "        report_end_round = end_round if end_round is not None else max_round_in_dataset\n",
    "\n",
    "        for col in display_df.columns:\n",
    "             if 'Rounds' in col:\n",
    "                 display_df[col] = display_df[col].apply(\n",
    "                     lambda x: f\">{report_end_round}\" if x == np.inf else (int(x) if pd.notna(x) else \"N/A\")\n",
    "                 )\n",
    "             elif 'Avg Acc' in col:\n",
    "                 display_df[col] = display_df[col].apply(\n",
    "                     lambda x: f\"{x:.2f}%\" if pd.notna(x) else \"N/A\"\n",
    "                 )\n",
    "\n",
    "        # Ensure index name is clear\n",
    "        display_df.index.name = 'Algorithm'\n",
    "        print(display_df)\n",
    "        print(\"-\" * (len(display_df.columns)*15)) # Adjust separator width\n",
    "\n",
    "\n",
    "        # Store for potential overall summary later\n",
    "        all_datasets_metrics[dataset] = metrics_for_current_dataset\n",
    "\n",
    "\n",
    "    return all_datasets_metrics # Optionally return all metrics\n",
    "\n",
    "# --- Configuration ---\n",
    "datasets_to_plot = [\"emnist\", \"shakespeare\", \"cifar100\"] # Select datasets\n",
    "algorithms_to_plot = [\"fedAvg\", \"fedAdam\", \"fedAdadb\"] # Select algorithms\n",
    "run_ids_to_process = [1, 2, 3, 4, 5] # Specify run numbers to average over\n",
    "base_results = \"results/official\" # Path containing dataset folders\n",
    "common_csv_file = \"c10_e4_validation_data.csv\" # *** VERIFY FILENAME CONSISTENCY ***\n",
    "col_to_plot = 'Accuracy'\n",
    "smoothing_window = 5      # Window size for smoothing the MEAN curve (e.g., 5). 1 = no smoothing.\n",
    "plot_start_round = 0      # Start plotting from this communication round\n",
    "plot_end_round = 2000     # Stop plotting at this communication round (set to None to plot all data)\n",
    "eval_frequency = 5        # *** IMPORTANT: Ensure this matches the frequency used during experiments ***\n",
    "output_plot_dir = \"figures/comparisons\" # Directory to save comparison plots\n",
    "\n",
    "# --- Metric Calculation Configuration ---\n",
    "metrics_thresholds_list = [35.0, 45.0] # Thresholds for Metric 2 (e.g., 75%, 85%)\n",
    "metric3_threshold = 45.0              # Threshold for starting Metric 3 calculation (e.g., 85%)\n",
    "metric_consistency = 3                # How many consecutive points needed >= threshold for Metric 2 (e.g., 3 -> current + 2 previous)\n",
    "\n",
    "# --- Generate the plots and calculate metrics ---\n",
    "calculated_metrics = plot_algorithm_comparison(\n",
    "    datasets=datasets_to_plot,\n",
    "    algorithms=algorithms_to_plot,\n",
    "    run_numbers=run_ids_to_process,\n",
    "    base_results_dir=base_results,\n",
    "    csv_filename_template=common_csv_file,\n",
    "    column_name=col_to_plot,\n",
    "    window_size=smoothing_window,\n",
    "    start_round=plot_start_round,\n",
    "    end_round=plot_end_round,\n",
    "    rounds_per_eval=eval_frequency,\n",
    "    output_base_dir=output_plot_dir,\n",
    "    metric_thresholds=metrics_thresholds_list,\n",
    "    post_avg_threshold=metric3_threshold,\n",
    "    metric_consistency_window=metric_consistency\n",
    ")\n",
    "\n",
    "print(\"\\n--- Plotting and Metrics Script Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a19ea-df34-4d55-bd76-a907b25ab720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "server_learning_rates = np.logspace(-3, 1, num=9)\n",
    "client_learning_rates = np.logspace(-3, 1, num=9)\n",
    "\n",
    "def plot_heatmap_from_csv(csv_file, server_learning_rates, client_learning_rates,\n",
    "                          results_path, dpi=300, font_size=8):\n",
    "    # Load the results matrix from the CSV file\n",
    "    results = np.genfromtxt(csv_file, delimiter=',')\n",
    "    basename = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    output_file = os.path.join(results_path, basename + \".jpg\")\n",
    "\n",
    "    # Format tick labels\n",
    "    s_labels = [f\"{lr:.3f}\" for lr in server_learning_rates]\n",
    "    c_labels = [f\"{lr:.3f}\" for lr in client_learning_rates]\n",
    "\n",
    "    # Set up figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    im = ax.pcolormesh(results, cmap='YlGn', shading='auto')\n",
    "    fig.colorbar(im, ax=ax, label='Accuracy')\n",
    "\n",
    "    # Ticks\n",
    "    ax.set_xticks(np.arange(len(s_labels)) + 0.5)\n",
    "    ax.set_yticks(np.arange(len(c_labels)) + 0.5)\n",
    "    ax.set_xticklabels(s_labels, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(c_labels)\n",
    "    ax.set_xlabel('Client Learning Rates')\n",
    "    ax.set_ylabel('Server Learning Rates')\n",
    "    ax.set_title('Validation Accuracy Grid')\n",
    "\n",
    "    # Normalize for color‐to‐RGBA lookup\n",
    "    norm = colors.Normalize(vmin=np.nanmin(results), vmax=np.nanmax(results))\n",
    "    cmap = plt.get_cmap('YlGn')\n",
    "\n",
    "    # Annotate each cell\n",
    "    for i in range(results.shape[0]):\n",
    "        for j in range(results.shape[1]):\n",
    "            val = results[i, j]\n",
    "            rgba = cmap(norm(val))\n",
    "            # Perceived brightness formula\n",
    "            brightness = 0.299*rgba[0] + 0.587*rgba[1] + 0.114*rgba[2]\n",
    "            text_color = 'white' if brightness < 0.5 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f\"{val:.2f}\",\n",
    "                    ha='center', va='center',\n",
    "                    color=text_color,\n",
    "                    fontsize=font_size,\n",
    "                    fontweight='bold')\n",
    "\n",
    "    # Tight layout so labels don’t get cut off\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save at high resolution\n",
    "    plt.savefig(output_file, dpi=dpi, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "results_path='results/official/emnist/fedAdadb/tuning'\n",
    "plot_heatmap_from_csv('results/official/emnist/fedAdadb/tuning/c5_e1_tuning_data.csv', server_learning_rates, client_learning_rates, results_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
